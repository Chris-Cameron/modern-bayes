% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Custom definitions
% To use this customization file, insert the line "\input{custom}" in the header of the tex file.

% Formatting




% Packages

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[page number]

 \usepackage{amssymb,latexsym}
\usepackage{amssymb,amsfonts,amsmath,latexsym,amsthm, bm}
%\usepackage[usenames,dvipsnames]{color}
%\usepackage[]{graphicx}
%\usepackage[space]{grffile}
\usepackage{mathrsfs}   % fancy math font
% \usepackage[font=small,skip=0pt]{caption}
%\usepackage[skip=0pt]{caption}
%\usepackage{subcaption}
%\usepackage{verbatim}
%\usepackage{url}
%\usepackage{bm}
\usepackage{dsfont}
\usepackage{multirow}
%\usepackage{extarrows}
%\usepackage{multirow}
%% \usepackage{wrapfig}
%% \usepackage{epstopdf}
%\usepackage{rotating}
%\usepackage{tikz}
%\usetikzlibrary{fit}					% fitting shapes to coordinates
%\usetikzlibrary{backgrounds}	% drawing the background after the foreground


% \usepackage[dvipdfm,colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue]{hyperref}
%\usepackage[colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue]{hyperref}
%%\usepackage{hyperref}
%\usepackage[authoryear,round]{natbib}


%  Theorems, etc.

%\theoremstyle{plain}
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{condition}[theorem]{Condition}
% \newtheorem{conditions}[theorem]{Conditions}

%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
%% \newtheorem*{unnumbered-definition}{Definition}
%\newtheorem{example}[theorem]{Example}
%\theoremstyle{remark}
%\newtheorem*{remark}{Remark}
%\numberwithin{equation}{section}




% Document-specific shortcuts
\newcommand{\btheta}{{\bm\theta}}
\newcommand{\bbtheta}{{\pmb{\bm\theta}}}

\newcommand{\commentary}[1]{\ifx\showcommentary\undefined\else \emph{#1}\fi}

\newcommand{\term}[1]{\textit{\textbf{#1}}}

% Math shortcuts

% Probability distributions
\DeclareMathOperator*{\Exp}{Exp}
\DeclareMathOperator*{\TExp}{TExp}
\DeclareMathOperator*{\Bernoulli}{Bernoulli}
\DeclareMathOperator*{\Beta}{Beta}
\DeclareMathOperator*{\Ga}{Gamma}
\DeclareMathOperator*{\TGamma}{TGamma}
\DeclareMathOperator*{\Poisson}{Poisson}
\DeclareMathOperator*{\Binomial}{Binomial}
\DeclareMathOperator*{\NormalGamma}{NormalGamma}
\DeclareMathOperator*{\InvGamma}{InvGamma}
\DeclareMathOperator*{\Cauchy}{Cauchy}
\DeclareMathOperator*{\Uniform}{Uniform}
\DeclareMathOperator*{\Gumbel}{Gumbel}
\DeclareMathOperator*{\Pareto}{Pareto}
\DeclareMathOperator*{\Mono}{Mono}
\DeclareMathOperator*{\Geometric}{Geometric}
\DeclareMathOperator*{\Wishart}{Wishart}

% Math operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\Vol}{Vol}

% Math characters
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\I}{\mathds{1}}
\newcommand{\V}{\mathbb{V}}

\newcommand{\A}{\mathcal{A}}
%\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Zcal}{\mathcal{Z}}
\renewcommand{\P}{\mathcal{P}}

\newcommand{\T}{\mathtt{T}}
\renewcommand{\emptyset}{\varnothing}


% Miscellaneous commands
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\matrixsmall}[1]{\bigl(\begin{smallmatrix}#1\end{smallmatrix} \bigr)}

\newcommand{\items}[1]{\begin{itemize} #1 \end{itemize}}

\newcommand{\todo}[1]{\emph{\textcolor{red}{(#1)}}}

\newcommand{\branch}[4]{
\left\{
	\begin{array}{ll}
		#1  & \mbox{if } #2 \\
		#3 & \mbox{if } #4
	\end{array}
\right.
}

% approximately proportional to
\def\app#1#2{%
  \mathrel{%
    \setbox0=\hbox{$#1\sim$}%
    \setbox2=\hbox{%
      \rlap{\hbox{$#1\propto$}}%
      \lower1.3\ht0\box0%
    }%
    \raise0.25\ht2\box2%
  }%
}
\def\approxprop{\mathpalette\app\relax}

% \newcommand{\approptoinn}[2]{\mathrel{\vcenter{
  % \offinterlineskip\halign{\hfil$##$\cr
    % #1\propto\cr\noalign{\kern2pt}#1\sim\cr\noalign{\kern-2pt}}}}}

% \newcommand{\approxpropto}{\mathpalette\approptoinn\relax}





\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Module 3: Introduction to the Normal Distribution},
  pdfauthor={Rebecca C. Steorts},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Module 3: Introduction to the Normal Distribution}
\author{Rebecca C. Steorts}
\date{}

\begin{document}
\frame{\titlepage}

\begin{frame}{Agenda}
\protect\hypertarget{agenda}{}
\begin{itemize}
\tightlist
\item
  The normal distribution
\item
  The variance versus precision
\item
  The re-parameterized normal distribution
\item
  Common properties
\item
  The normal-improper model
\item
  The normal-normal model
\end{itemize}
\end{frame}

\begin{frame}{Normal distribution}
\protect\hypertarget{normal-distribution}{}
The normal distribution \(\N(\mu,\sigma^2)\)

\begin{itemize}
\tightlist
\item
  with mean \(\mu\in\R\) and variance \(\sigma^2 > 0\) - (standard
  deviation \(\sigma =\sqrt{\sigma^2}\)) has p.d.f.
  \[\N(x\mid\mu,\sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}(x-\mu)^2\Big) \]
  for \(x\in\R\).
\end{itemize}

It is often more convenient to write the p.d.f.~in terms of the
\textbf{precision}, or inverse variance, \(\lambda = 1/\sigma^2\) rather
than the variance.
\end{frame}

\begin{frame}{Re-parameterized Normal}
\protect\hypertarget{re-parameterized-normal}{}
In this parametrization, the p.d.f.~is
\[\N(x\mid\mu,\lambda^{-1}) =\sqrt{\frac{\lambda}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda (x-\mu)^2\big) \]
since \(\sigma^2 = 1/\lambda =\lambda^{-1}\).

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/normal.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Normal distribution with various choices of $\mu$ and $\sigma$.}
\end{figure}
\end{frame}

\begin{frame}{Normality?}
\protect\hypertarget{normality}{}
\begin{itemize}
\tightlist
\item
  The central limit theorem (CLT) states that the sum of a large number
  of independent random variables tends to be approximately normally
  distributed.\\
\item
  Real world data often appears approximately normal.
\end{itemize}
\end{frame}

\begin{frame}{Normality?}
\protect\hypertarget{normality-1}{}
\begin{itemize}
\item
  Human heights and other body measurements,
\item
  Cumulative hydrologic measures such as annual rainfall or monthly
  river discharge,
\item
  Errors in astronomical or physical observations,
\item
  Diffusion of a substance in a liquid or gas.
\item
  Some things are products of many independent variables (rather than
  sums), and in such cases the logarithm will be approximately normal
  since it is a sum of many independent variables
\end{itemize}

Example: stock market indices, due to the effect of compound interest.
\end{frame}

\begin{frame}{Properties of the Normal distribution}
\protect\hypertarget{properties-of-the-normal-distribution}{}
\begin{itemize}
\item Mean, median, and mode are all the same ($\mu$)
\item Symmetric about the mean
\item 95\% probability within $\pm 1.96\sigma$ of the mean (roughly, $\pm 2\sigma$)
\item If $X\sim\N(\mu,\sigma^2)$ and $Y\sim\N(m,s^2)$ independently, then
\begin{align}\label{equation:linear-combination}
a X + b Y\sim\N(a\mu + b m,\,a^2\sigma^2+ b^2 s^2).
\end{align}
\item Careful: \texttt{rnorm}, \texttt{dnorm}, \texttt{pnorm}, and \texttt{qnorm} in \texttt{R} take the mean and \textcolor{red}{standard deviation} $\sigma$ as arguments (not mean and variance $\sigma^2$). For example, \texttt{rnorm(n,m,s)} generates $n$ normal random variables from $\N(m,s^2)$.
\end{itemize}
\end{frame}

\begin{frame}{\textcolor{blue}{Normal-Improper}}
\protect\hypertarget{section}{}
\[ X_1,\dotsc,X_n \mid \theta \iid\N(\theta,\sigma^2). \]

Assume the prior on \(\theta\) is constant over the real line. We can
write this as \(p(\theta) \propto 1,\) where
\(-\infty < \theta < \infty.\)

\textcolor{blue}{Observe that this density is improper as any constant density over the real line integrates to $\infty.$}

Derive the posterior
distribution.\footnote{Please observe that the posterior is not conjugate to the prior in this situation, so it's very important to make sure that the posterior is a proper distribution.}
\end{frame}

\begin{frame}{Solution}
\protect\hypertarget{solution}{}
\begin{align}
p(\theta \mid x_{1:n}) 
&\propto
\textcolor{blue}{\N(x_{1:n} \mid \theta,\sigma^2)} \times 1 \\
& \propto (\frac{\ell}{2\pi})^{n/2}\exp\big(-\tfrac{1}{2}\ell \sum_i(x_i-\theta)^2\big)\notag\\
& \propto \exp\big(-\tfrac{1}{2}\ell \sum_i(x_i -\bar{x} + \bar{x} - \theta)^2\big)\notag\\
& \propto \exp\big(-\tfrac{1}{2}\ell \sum_i(x_i -\bar{x})^2)
\exp\big(-\tfrac{1}{2}\ell \sum_i( \bar{x} - \theta)^2\big)\notag\\
&\propto 
\exp\big(-\tfrac{1}{2}\ell \sum_i( \bar{x} - \theta)^2\big) \\
&= \textcolor{red}{\exp\big(-\tfrac{n \ell}{2} (\theta - \bar{x} )^2\big)}
= \textcolor{blue}{\N(\theta \mid \bar{x}, (n\ell)^{-1}).}
\end{align}

This implies that
\[\theta \mid x_{1:n} \sim N(\bar{x}, (n\ell)^{-1}) = N(\bar{x}, \sigma^2/n)\]
\end{frame}

\begin{frame}{Commonly asked questions on the Normal-Improper
derivation}
\protect\hypertarget{commonly-asked-questions-on-the-normal-improper-derivation}{}
\begin{enumerate}
\tightlist
\item
  Why do we add and subtract \(\bar{x}\)?
\end{enumerate}

This trick makes the cross product term 1.

\begin{enumerate}
\setcounter{enumi}{1}
\tightlist
\item
  Why is the cross product term 1? The cross product term is:
\end{enumerate}

\begin{align*}
&\exp\big(-\tfrac{2}{2}\ell)\sum_i(x_i -\bar{x})(\bar{x} - \theta)\\
&=\exp\big(-\ell)(\bar{x} - \theta)\sum_i(x_i -\bar{x})\\
&=e^{0}=1.
\end{align*}
\end{frame}

\begin{frame}{Normal-Normal}
\protect\hypertarget{normal-normal}{}
\[ X_1,\dotsc,X_n \mid \theta \iid\N(\theta,\lambda^{-1}). \] Assume the
precision \(\lambda = 1/\sigma^2\) is known and fixed, and \(\theta\) is
given a \(\N(\mu_0,\lambda_0^{-1})\) prior:
\[\btheta \sim \N(\mu_0,\lambda_0^{-1})\] i.e.,
\(p(\theta) = \N(\theta\mid \mu_0,\lambda_0^{-1})\). This is sometimes
referred to as a \textbf{Normal--Normal} model.
\end{frame}

\begin{frame}{Posterior derivation}
\protect\hypertarget{posterior-derivation}{}
We begin with the \textbf{likelihood} of the normal distribution.
\textcolor{blue}{We work with proportionality with respect to $\theta$ as we will combine the **likelihood** and **prior** to find the posterior, where the parameter of interest is $\theta.$}

\textcolor{red}{For any $x$ and $\ell$},
\begin{align}\label{equation:prop}
\N(x\mid\theta,\ell^{-1})
& =\sqrt{\frac{\ell}{2\pi}}\exp\big(-\tfrac{1}{2}\ell (x-\theta)^2\big)\notag\\
&\underset{\theta}{\propto} \exp\big(-\tfrac{1}{2}\ell (x^2 - 2 x \theta +\theta^2)\big)\notag\\
&\textcolor{red}{\underset{\theta}{\propto} \exp\big(\ell x \theta -\tfrac{1}{2}\ell\theta^2)\big)}.
\end{align}

Note: we drop the \textbf{constant term} and we will do this often when
working with the normal distribution.
\end{frame}

\begin{frame}{Posterior derivation (continued)}
\protect\hypertarget{posterior-derivation-continued}{}
We now consider the \textbf{prior} distribution on \(\theta.\)

Due to the symmetry of the normal p.d.f.,
\begin{align}\label{equation:prior}
\N(\theta\mid\mu_0,\lambda_0^{-1}) =
\sqrt{\frac{\lambda_o}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda_o (\theta - \mu_o)^2\big)\notag\\
= \sqrt{\frac{\lambda_o}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda_o (\mu_o - \theta)^2\big)\notag\\
= \N(\mu_0\mid\theta,\lambda_0^{-1})
\underset{\theta}{\propto}\textcolor{red}{\exp\big(\lambda_0\mu_0\theta-\tfrac{1}{2}\lambda_0\theta^2\big)},
\end{align} where \(x=\mu_0\) and \(\ell=\lambda_0\).
\end{frame}

\begin{frame}{Posterior derivation (continued)}
\protect\hypertarget{posterior-derivation-continued-1}{}
Let \[L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.\]
\begin{align*}
p(\theta|x_{1:n})&\propto p(\theta) p(x_{1:n}|\theta) \\
&= \N(\theta\mid\mu_0,\lambda_0^{-1})\prod_{i = 1}^n\N(x_i\mid \theta,\lambda^{-1})\\
&\overset{\text{(a)}}{\propto} \exp\big(\lambda_0\mu_0\theta-\tfrac{1}{2}\lambda_0\theta^2\big)
         \exp\big(\lambda (\textstyle\sum x_i) \theta -\tfrac{1}{2}n\lambda\theta^2\big)\\
&= \exp\Big((\lambda_0\mu_0+\lambda\textstyle\sum x_i)\theta-\tfrac{1}{2}(\lambda_0+ n\lambda)\theta^2\Big)\\
&= \exp(L M\theta-\tfrac{1}{2}L\theta^2)\\
&\overset{\text{(b)}}{\propto} \N(M\mid\theta,L^{-1}) =\N(\theta\mid M,L^{-1}),
\end{align*} where step (a) uses Equations \ref{equation:prop} and
\ref{equation:prior}, and step (b) uses Equation \ref{equation:prop}
with \(x=M\) and \(\ell=L\).
\end{frame}

\begin{frame}{Posterior derivation (continued)}
\protect\hypertarget{posterior-derivation-continued-2}{}
Recall \[L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.\]

It turns out that the posterior is

\begin{equation}
\label{equation:posterior}
\textcolor{red}{\btheta|x_{1:n}\, \sim \,\N(M,L^{-1})}
\end{equation}

i.e., \(p(\theta|x_{1:n}) =\N(\theta\mid M,L^{-1})\).

Thus, the normal distribution is, itself, a conjugate prior for the mean
of a normal distribution with known precision.
\end{frame}

\begin{frame}{Heights of Adult Humans}
\protect\hypertarget{heights-of-adult-humans}{}
\begin{itemize}
\tightlist
\item
  Heights tend to be normally distributed because there are many
  independent genetic and environmental factors which contribute
  additively to overall height
\item
  This leads to a normal distribution due to the central limit theorem.

  \begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-female-male.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Estimated densities of the heights of Dutch women and Dutch men based on a sample of 695 women and 562 men.}
  \label{figure:heights}
  \end{figure}
\end{itemize}
\end{frame}

\begin{frame}{Heights of Adult Humans}
\protect\hypertarget{heights-of-adult-humans-1}{}
\begin{itemize}
\tightlist
\item
  Consider combined distribution of heights (pooling females and males
  together). Would this be normal?
\item
  It is thought that such data is bimodal (having two maxima). Is it
  really bimodal? (See, Schilling et al.~(2002))

  \begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-combined.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Estimated density for Dutch women and men together, assuming there is an equal proportion of women and men in the population.}
  \label{figure:heights-combined}
  \end{figure}
\end{itemize}
\end{frame}

\begin{frame}{Heights of Adult Humans, Combined}
\protect\hypertarget{heights-of-adult-humans-combined}{}
At a glance, while the heights of women and men separately do appear to
be roughly normally distributed, the combined distribution does not look
bimodal. How could we test whether it is bimodal in a more precise way?
\end{frame}

\begin{frame}{Our Assumptions}
\protect\hypertarget{our-assumptions}{}
\begin{itemize}
\tightlist
\item
  Assume female heights and male heights are each normally distributed.
\item
  \textcolor{red}{Assume both female heights and male heights have different means but the same standard deviation.}
\item
  \textcolor{red}{Assume that there is an equal proportion of women and men in the population.}
\item
  Then, it is known that the combined distribution is bimodal if and
  only if the difference between the means is greater than twice the
  standard deviation (Helguerro, 1904).
\end{itemize}
\end{frame}

\begin{frame}{Model}
\protect\hypertarget{model}{}
In mathematical notation: Assume the female heights are
\[X_1,\dotsc,X_k\iid\N(\theta_f,\sigma^2),\] where \(k=695\), the male
heights are \[Y_1,\dotsc,Y_\ell\iid\N(\theta_m,\sigma^2),\] where
\(\ell=562\), and the p.d.f.~of the combined distribution of heights is
\[\tfrac{1}{2}\N(x\mid\theta_f,\sigma^2)+\tfrac{1}{2}\N(x\mid\theta_m,\sigma^2). \]
(This is an example of what is called a two-component \term{mixture}
distribution.)
\end{frame}

\begin{frame}{Model}
\protect\hypertarget{model-1}{}
Let's put independent normal priors on \(\theta_f\) and \(\theta_m\):
\[ p(\theta_f,\theta_m) = p(\theta_f) p(\theta_m) 
=\N(\theta_f\mid \mu_{0,f},\sigma_0^2)\N(\theta_m\mid \mu_{0,m},\sigma_0^2).\]

\begin{itemize}
\tightlist
\item
  Assume \(\sigma^2\) is known.
\item
  For the purposes of this example, let's use \(\sigma=8\) centimeters
  (about 3 inches).
\item
  Based on common knowledge of typical human heights, let's choose the
  prior parameters (a.k.a. hyperparameters) as follows:

  \begin{center}
  \begin{tabular}{cll}
  $\mu_{0,f}$ & (mean of prior on female mean ht) & \text{165 cm ($\approx$ 5 ft, 5 in)}\\
  $\mu_{0,m}$ & (mean of prior on male mean ht) & \text{178 cm ($\approx$ 5 ft, 10 in)}\\
  $\sigma_0$ & (std.\ dev.\ of priors on mean ht) & \text{15 cm ($\approx$ 6 in)}\\
  \end{tabular}
  \end{center}
\end{itemize}
\end{frame}

\begin{frame}{Bimodal Fact}
\protect\hypertarget{bimodal-fact}{}
It is known (Helguerro, 1904) that the combined distribution is bimodal
if and only if \[ |\theta_f-\theta_m|>2\sigma. \] So, to address our
question of interest (``Is human height bimodal?'\,'), we would like to
compute the posterior probability that this is the case, i.e., we want
to know
\[ \Pr(\text{bimodal}\mid \text{data}) = \Pr\big(|\btheta_f-\btheta_m|>2\sigma \mid x_{1:k},y_{1:\ell}\big).\]
\end{frame}

\begin{frame}{Results}
\protect\hypertarget{results}{}
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-prior-posterior.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Priors and posteriors for the mean heights of Dutch women and men.}
  \label{figure:heights-prior-posterior}
\end{figure}
\end{frame}

\begin{frame}{Results (continued)}
\protect\hypertarget{results-continued}{}
We can compute the posteriors for \(\theta_f\) and \(\theta_m\) using
\textcolor{red}{Equation \ref{equation:posterior}} for each of them,
independently. Figure \ref{figure:heights-prior-posterior} shows the
priors and posteriors.

\begin{itemize}
\item Sample means: $\bar x = 168.0$ cm (5 feet 6.1 inches) for females, and $\bar y = 181.4$ cm (5 feet 11.4 inches) for males.
\item Posterior means: $M_f = 168.0$ cm for females, and $M_m = 181.4$ cm for males. (Essentially identical to the sample means, due to the relatively large sample size and relatively weak prior.)
\item Posterior standard deviations: $1/\sqrt{L_f} = 0.30$ cm and $1/\sqrt{L_m} = 0.34$ cm.
\end{itemize}
\end{frame}

\begin{frame}{Results (continued)}
\protect\hypertarget{results-continued-1}{}
By Equation \ref{equation:linear-combination} (a linear combination of
independent normals is normal),
\[\btheta_m-\btheta_f\mid x_{1:k},y_{1:\ell}\,\sim\,\N(M_m-M_f,\, L_m^{-1} + L_f^{-1}) = \N(13.4,0.45^2) \]
so we can compute \(\Pr(\text{bimodal}\mid \text{data})\) using the
normal c.d.f.~\(\Phi\): \begin{align*}
&\Pr(\text{bimodal}\mid \text{data})= \Pr\big(|\btheta_m-\btheta_f|>2\sigma \mid x_{1:k},y_{1:\ell}\big) \\
&= \textcolor{red}{\Pr(\btheta_m-\btheta_f < -2 \sigma) + \Pr(\btheta_m-\btheta_f > 2 \sigma)} \\
&= \Phi(-2\sigma\mid 13.4,0.45^2)
+ \big(1-\Phi(2\sigma\mid 13.4,0.45^2)\big)\\
& = 6.1\times 10^{-9}.
\end{align*}

Intuitive interpretation: The posteriors are about 13 or 14 centimeters
apart, which is under the \(2\sigma = 16\) threshold for bimodality, and
they are sufficiently concentrated that the posterior probability of
bimodality is essentially zero.
\end{frame}

\begin{frame}{Takeaways}
\protect\hypertarget{takeaways}{}
\begin{itemize}
\tightlist
\item
  We reviewed the univariate normal distribution and properties of it
  (such as symmetry about the mean).
\item
  We discussed types of data that empirically have a normal distribution
  to motivate its usage.
\item
  We dervied the Normal-Improper model, where we observed our first
  improper prior.
\item
  We derived the Normal-Normal model.
\item
  We considered an application of human heights from Schilling et
  al.~(2002), where we showed that the posterior probability of
  bimodality was zero (regarding a plot of combined heights).
\end{itemize}
\end{frame}

\begin{frame}{Detailed Takeaways for Exam}
\protect\hypertarget{detailed-takeaways-for-exam}{}
\begin{itemize}
\tightlist
\item
  Working with the univariate normal distribution
\item
  Knowing the precision and variance relationship
\item
  Knowing the shape of the normal distribution
\item
  Knowing the CLT
\item
  Knowing what type of data is approximately normal and what data is not
\item
  Knowing properties of the normal that we will often work with such as
  symmetry about the mean
\item
  Being able to derive the posterior of the normal-improper model
\item
  Understanding that a non-informative prior is highly informative and
  why.
\item
  Being able to derive the normal-normal posterior distribution
\item
  Knowing how to write out the posterior mean and variance intuitively
\end{itemize}
\end{frame}

\begin{frame}{Detailed Takeaways for Exam (Continued)}
\protect\hypertarget{detailed-takeaways-for-exam-continued}{}
\begin{itemize}
\tightlist
\item
  You should be able to work with a two-component mixture model (as in
  the case study)
\item
  Suppose I provide you with a case study such as the one on human
  heights. You should be able to explain why it makes sense to model the
  data as normal and what paramaters would be reasonable given the
  description.
\item
  Given as case study, you should be able to incorporate a pilot (or
  prior study) into your prior distribution and back up how you
  incorporate it.
\item
  You should be able to update the posterior accordingly.
\item
  You be able to state benefits of your model specification and
  weaknesses.
\item
  You should be able to state a better model that would be more
  realistic once we have covered the normal-normal-gamma model (module
  4).
\item
  You should be able to discuss sensitity of the posterior analysis for
  any hyper-parameters.
\end{itemize}
\end{frame}

\begin{frame}{Exercise}
\protect\hypertarget{exercise}{}
Recall that Recall \[L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.\]

What happens to the posterior mean and the precision as
\(n \rightarrow \infty.\)
\end{frame}

\begin{frame}{Exercise}
\protect\hypertarget{exercise-1}{}
Let's consider the posterior mean. \[
\begin{aligned}
M &=\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda} \\
&=\frac{\lambda_0\mu_0+\lambda n \bar{x}}{\lambda_0+ n\lambda} \\
&=\frac{\lambda_0\mu_0/n+\lambda  \bar{x}}{\lambda_0/n+ \lambda} \\
\end{aligned}
\]

As \(n \rightarrow \infty,\)

\[M \rightarrow \lambda  \bar{x}/\lambda = \bar{x}.\]
\end{frame}

\begin{frame}{Exercise}
\protect\hypertarget{exercise-2}{}
Let's consider the posterior variance.

\[L^{-1} =\frac{1}{\lambda_0+ n\lambda}\]

As \(n \rightarrow \infty,\)

\[L^{-1} = \frac{1}{\lambda_0+ n\lambda} = \frac{1/n}{\lambda_0/n+ \lambda} \approx \frac{1}{n \lambda} \rightarrow 0 \]
\end{frame}

\begin{frame}{Interpretation}
\protect\hypertarget{interpretation}{}
What can we learn from this example?

If our sample size is large enough (for any application or real world
example), then

\begin{enumerate}
\item the posterior mean will tend to the the sample mean.
\item the posterior variance will tend to 0. 
\end{enumerate}
\end{frame}

\begin{frame}{Module 3 Class Notes}
\protect\hypertarget{module-3-class-notes}{}
Module 3 Class Notes can be found here:

\url{https://github.com/resteorts/modern-bayes/tree/master/lecturesModernBayes20/lecture-3/03-class-notes}
\end{frame}

\end{document}
